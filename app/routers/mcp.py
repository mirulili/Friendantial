import os

import httpx
from fastapi import APIRouter, HTTPException, Query, Request

from app.llm.llm_clients import AbstractLLMClient  # ì¶”ìƒ í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from app.llm.prompts import ANALYST_PERSONA, FRIEND_PERSONA  # í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ì„í¬íŠ¸
from app.services.rag import rag_engine
from app.services.sentiment import fetch_news_titles
from app.utils.caching import cached_llm_generation  # ìºì‹± ë°ì½”ë ˆì´í„° ì„í¬íŠ¸

router = APIRouter(
    prefix="/mcp",
    tags=["mcp-features"],
)


@cached_llm_generation(prefix="llm-generated-text", ttl_days=1)
async def generate_text_with_persona(
    request: Request,
    *,
    persona_name: str,
    user_prompt: str,
    llm_client: AbstractLLMClient,
) -> str:
    """
    ì§€ì •ëœ í˜ë¥´ì†Œë‚˜ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMìœ¼ë¡œë¶€í„° í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì‚¬ìš©í•  ëª¨ë¸ì€ LLM_MODEL_NAME í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜µë‹ˆë‹¤.
    """
    llm_client: AbstractLLMClient = request.app.state.llm_client
    if not llm_client:
        raise HTTPException(
            status_code=503, detail="LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )

    persona_map = {
        "friend": FRIEND_PERSONA,
        "analyst": ANALYST_PERSONA,
    }
    system_prompt = persona_map.get(persona_name)
    if not system_prompt:
        raise HTTPException(
            status_code=400, detail=f"ì•Œ ìˆ˜ ì—†ëŠ” í˜ë¥´ì†Œë‚˜: {persona_name}"
        )

    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì´ë¦„ì„ ì½ì–´ì˜´
    # ì—†ìœ¼ë©´ 'gpt-4-turbo'ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
    model_name = os.getenv("LLM_MODEL_NAME", "gpt-4-turbo")

    # ì¶”ìƒí™”ëœ í´ë¼ì´ì–¸íŠ¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ìƒì„±
    generated_text = await llm_client.generate_chat_completion(
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        model=model_name,
    )
    return generated_text


@router.get("/prompts/friend")
def get_friend_prompt():
    return {"system_prompt": FRIEND_PERSONA}


@router.get("/prompts/analyst")
def get_analyst_prompt():
    return {"system_prompt": ANALYST_PERSONA}


@router.get("/ask/{stock_code}", summary="ì¢…ëª© ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ (RAG)")
async def ask_about_stock(
    request: Request,
    stock_code: str,
    question: str = Query(..., description="ì§ˆë¬¸ ë‚´ìš© (ì˜ˆ: ì™œ ë–¨ì–´ì ¸?)"),
    persona: str = Query("friend", description="friend ë˜ëŠ” analyst"),
):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ (RAG ì ìš©).
    """
    # 1. ì¢…ëª©ëª… ì¡°íšŒ
    stock_name = stock_code
    async with httpx.AsyncClient() as client:
        try:
            # app.stateì— ë“±ë¡ëœ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš© (main.pyì—ì„œ ë“±ë¡ë¨)
            if hasattr(request.app.state, "lookup_stock_info"):
                stock_info = await request.app.state.lookup_stock_info(
                    client, request.app.state.redis, stock_code
                )
                if stock_info:
                    stock_name = stock_info.get("itmsNm", stock_name)
        except Exception:
            pass  # ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì½”ë“œëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©

    # 2. ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ (ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•)
    # RAGë¥¼ ìœ„í•´ í‰ì†Œë³´ë‹¤ ë§ì€ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ (ì˜ˆ: 15ê°œ)
    async with httpx.AsyncClient() as client:
        news_titles = await fetch_news_titles(client, stock_name, limit=15)

    if not news_titles:
        return {"answer": "ê´€ë ¨ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í•´ì„œ ë‹µë³€í•˜ê¸° ì–´ë ¤ì›Œ ğŸ˜¢"}

    # 3. RAG: ë²¡í„° DBì— ì €ì¥ ë° ê²€ìƒ‰
    # (1) ì§€ì‹ ì €ì¥ (Ingestion)
    rag_engine.create_collection(stock_code, news_titles)

    # (2) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (Retrieval)
    relevant_news = rag_engine.query(stock_code, question, n_results=5)

    # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Context Stuffing)
    context_text = "\n".join([f"- {title}" for title in relevant_news])

    persona_prompt = FRIEND_PERSONA if persona == "friend" else ANALYST_PERSONA

    system_msg = f"""
    {persona_prompt}
    
    [ì§€ì‹œì‚¬í•­]
    ì‚¬ìš©ìëŠ” '{stock_name}({stock_code})'ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.
    ì•„ë˜ ì œê³µëœ 'ìµœì‹  ë‰´ìŠ¤' ë‚´ìš©ì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ë‰´ìŠ¤ì— ì—†ëŠ” ë‚´ìš©ì€ "ë‰´ìŠ¤ì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ë‹¤"ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.
    """

    user_msg = f"""
    [ìµœì‹  ë‰´ìŠ¤]
    {context_text}
    
    [ì§ˆë¬¸]
    {question}
    """

    # 5. LLM ë‹µë³€ ìƒì„±
    llm_client: AbstractLLMClient = request.app.state.llm_client
    model_name = os.getenv("LLM_MODEL_NAME", "gpt-4-turbo")

    answer = await llm_client.generate_chat_completion(
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        model=model_name,
    )

    return {
        "stock": stock_name,
        "question": question,
        "context_used": relevant_news,  # ì–´ë–¤ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ ëª…ì‹œ
        "answer": answer,
    }
